<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
<head>
	<!-- 支持中文 -->
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Gu Wang</title>
<style type="text/css">
	body
	{
		width:1400px;
		text-align: center;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size:16px;
		background-color: #FFF;
	}
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	table
	{
		padding: 5px;
	}

/* 选颜色 https://www.color-hex.com/color-names.html */
/* https://www.htmlcsscolor.com/hex/3B3B3B */
	table.pub_table,td.pub_td1,td.pub_td2
	{
		border-collapse: collapse;
		border-bottom: 0px solid #9B9B9B;
		padding-bottom: 10px;
		padding-top: 10px;
		padding-left: 10px;
		width: 1100px;
	}
	td.pub_td1
	{
		width:100px;
	}
	td.pub_td2
	{
		font-size: 16px;
	}
	td.sub_heading
	{
		color: #3B3B3B;
		font-weight: 700;
		/* font-size:20px; */
		font-size:120%;
	}
	tr {
		background-color: #FFF;
	}

	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #9B9B9B;
		height: 128px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #000;
		margin-bottom: 20px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
		font:11px helvetica,sans-serif;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
		font:11px helvetica,sans-serif;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
	}
	a:link,a:visited
	{
		/* color: #1367a7; */ /* 蓝色 */
		color: #990000; /* 深红色 990000 8b2323*/
		font-family: Tahoma, Geneva, sans-serif; /* 加粗 */
		text-decoration: none;
	}
	.section_div {
		background-color: #FFF;
		padding: 10px 10px 10px 10px;
		margin: 10px 10px 10px 10px;
		//border: 1px solid #AAA;
	}
	body {
		background-color: #FFF;
	}
	#personal_info {
		background-color: #FFF;
	}
	img.teaser_img {
		/* width: 256px; */
		width: 100px;
		display: block;
    margin-left: auto;
    margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
	}
	img.photo_of_me {
		border-radius: 20px;
	}
	div.teaser_img_div {
		/* width: 286px; */
		width: 100px;
	}


</style>

<!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24665197-4', 'auto');
  ga('send', 'pageview');

</script> -->
<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>

<link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>


<body>
	<div id="container">

	<div class='section_div'>
	<table id="personal_info">
	<tr>
	<td><img class="photo_of_me" src="images/guwang/guwang.jpg" width=180px style="border: 1px solid black; float:left; margin-right:15px"/></td>
	<td>
	<div id="DocInfo">
		<h1>Gu Wang (王谷)</h1>
		wangg12 at tsinghua dot org dot cn<br><br>
		<a href="https://scholar.google.com/citations?user=htu3c7wAAAAJ">Google Scholar</a> /
		<a href="https://orcid.org/0000-0002-0759-0782">ORCID</a> </a> /
		<a href="https://www.semanticscholar.org/author/29644358">Semantic Scholar</a> </a> /
		<a href="https://www.aminer.cn/profile/5614478245cedb3397a3fae4">Aminer</a> </a> /
		<a href="https://www.researchgate.net/profile/Gu_Wang">Research Gate</a> </a> /
		<a href="https://github.com/wangg12">GitHub</a> </a>
		<!-- https://orcid.org/0000-0002-0759-0782 -->
	</div><br>
	</td>
	</tr>
	</table>

	<h2>About Me</h2>
	I received my B.E. degree in 2016 and Ph.D. degree in 2022 from <a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html">Department of Automation</a>,
	<a href="http://www.sist.tsinghua.edu.cn/docinfo_eng/index.jsp">School of Information Science and Technology</a>,
	<a href="http://www.tsinghua.edu.cn/publish/newthuen/index.html">Tsinghua University</a>, under the supervision of Prof. <a href="http://xiangyangji.com/index.php">Xiangyang Ji</a>.
	My research interests lie in Computer Vision and Deep Learning.
	During my Ph.D., I focused on 6D object pose estimation with RGB based methods.
	I am also interested in other vision and robotics related perception problems in both 2D and 3D worlds.
	</div>
	<hr>

<!-- <hr> -->

<div class='section_div'>
<h2>News</h2>
<li>[04/2022] I am co-organizing the <a href="http://cmp.felk.cvut.cz/sixd/workshop_2022/">7th International Workshop on Recovering 6D Object Pose</a> and <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2022/">BOP Challenge 2022</a> @ ECCV 22.</li>
<li>[02/2022] I joined JD.com as a member of DMT (Doctoral Management Trainee).</li>
<li>[12/2021] PhD defended.</li>
<li>[08/2020] Our work CDPNv2 won the Best Method on Single Dataset Award of <a href="https://bop.felk.cvut.cz/leaderboards/">BOP Challenge 2020</a>. </li>
<li>[10/2019] Our work CDPN won the Best RGB-Only Method Award of <a href="https://bop.felk.cvut.cz/leaderboards/">BOP Challenge 2019</a>. </li>
</div>
<hr>


<!-- <div class='section_div'>
<h2>Teaching</h2>
<li><a href="xx">TODO</a> (Spring Term 2020)<br></li>
<li><a href="TODO">TODO</a> (Autumn Term 2019)<br></li>
</div>
<hr> -->


<!-- <div class='section_div'>
<h2>Services</h2>
<li>TODO</li>
<li>TODO</li>
</div>
<hr> -->



	<div class='section_div'>

	<h2>Publications</h2>

	<table class="pub_table">

	For all publications please check my
	<a href="https://scholar.google.com/citations?user=htu3c7wAAAAJ">Google Scholar</a> /
	<a href="https://www.semanticscholar.org/author/29644358">Semantic Scholar</a> </a> /
	<a href="https://www.researchgate.net/profile/Gu_Wang">Research Gate</a> </a>.<br>
	(* indicates equal contribution)<br>

  <!-- <tr><td class="sub_heading">Recent Selected Papers<hr></td></tr> -->
	<tr>
		<td class="pub_td1">
			<div class="teaser_img_div"><a href="https://arxiv.org/abs/2207.08082"><img class="teaser_img"
						src='images/paper/2022eccv_catre.png' /></a></div>
		</td>
		<td class="pub_td2"><b>CATRE: Iterative Point Clouds Alignment for Category-level Object Pose Refinement</b><br>
			<a href="https://lliu-xingyu.github.io/">Xingyu Liu*</a>,
			<b> Gu Wang*</b>,
			<a href="http://yili.vision/"> Yi Li</a>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a><br>
			<i>In European Conference on Computer Vision (ECCV), 2022</i>.<br>
			[<a href="todo">Paper</a>][<a href="https://arxiv.org/abs/2207.08082">arXiv</a>]
			[<a href="https://github.com/THU-DA-6D-Pose-Group/CATRE">Code</a>]
			[<a href="./paper/catre_eccv2022.html">bibtex</a>]
		</td>
	</tr>

	<tr>
		<td class="pub_td1">
			<div class="teaser_img_div"><a href="https://doi.org/10.1109/TPAMI.2021.3136301"><img class="teaser_img"
						src='images/paper/self6dpp_tpami.png' /></a></div>
		</td>
		<td class="pub_td2"><b>Occlusion-Aware Self-Supervised Monocular 6D Object Pose Estimation</b><br>
			<b>Gu Wang</b>*,
			<a href="https://campar.in.tum.de/Main/FabianManhardt"> Fabian Manhardt*</a>,
			<a href="https://lliu-xingyu.github.io/"> Xingyu Liu</a>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/senior-research-scientists/federico-tombari/"> Federico Tombari</a><br>
			<i>In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021</i>.<br>
			[<a href="https://doi.org/10.1109/TPAMI.2021.3136301">Paper</a>]
			[<a href="https://arxiv.org/abs/2203.10339">arXiv</a>]
			[<a href="https://github.com/THU-DA-6D-Pose-Group/self6dpp">Code</a>]
			[<a href="./paper/self6dpp_tpami.html">bibtex</a>]
		</td>
	</tr>

	<tr>
		<td class="pub_td1">
			<div class="teaser_img_div"><a href="https://arxiv.org/abs/2108.08367"><img class="teaser_img"
						src='images/paper/2021iccv_so_pose_3.png' /></a></div>
		</td>
		<td class="pub_td2"><b>SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation</b><br>
			<a href="https://scholar.google.com/citations?user=HSlGGvwAAAAJ">Yan Di</a>,
			<a href="https://campar.in.tum.de/Main/FabianManhardt"> Fabian Manhardt</a>,
			<b> Gu Wang</b>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/cv-nassir-navab/nassir-navab/"> Nassir Navab</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/senior-research-scientists/federico-tombari/"> Federico Tombari</a><br>
			<i>In IEEE International Conference on Computer Vision (ICCV), 2021</i>.<br>
			[<a href="https://arxiv.org/abs/2108.08367">Paper</a>]
			[<a href="https://github.com/shangbuhuan13/SO-Pose">Code</a>]
			[<a href="./paper/so_pose_iccv2021.html">bibtex</a>]
		</td>
	</tr>

	<tr>
		<td class="pub_td1">
			<div class="teaser_img_div"><a href="https://arxiv.org/abs/2102.12145"><img class="teaser_img"
						src='images/paper/2021cvpr_gdrn.png' /></a></div>
		</td>
		<td class="pub_td2"><b>GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation</b><br>
			<b>Gu Wang</b>,
			<a href="https://campar.in.tum.de/Main/FabianManhardt"> Fabian Manhardt</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/senior-research-scientists/federico-tombari/"> Federico Tombari</a>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a><br>
			<i>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</i>.<br>
			[<a href="https://arxiv.org/pdf/2102.12145.pdf">Paper</a>]
			[<a href="https://git.io/GDR-Net">Code</a>]
			[<a href="./paper/gdrn_cvpr2021.html">bibtex</a>]
		</td>
	</tr>

	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2003.05848"><img class="teaser_img" src='images/paper/2020_cpspp.png'/></a></div></td>
		<td class="pub_td2"><b>CPS++: Improving Class-level 6D Pose and Shape Estimation From Monocular Images With Self-Supervised Learning</b><br>
			<a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
			<b> Gu Wang</b>,
			<a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/"> Benjamin Busam</a>,
			<a href="https://campar.in.tum.de/Main/ManuelNickel"> Manuel Nickel</a>,
			<a href="https://www.linkedin.com/in/sven-meier-4ba83342/"> Sven Meier</a>,
			<a href="https://scholar.google.co.uk/citations?user=rUSxe4AAAAAJ&hl=en"> Luca Minciullo</a>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/cv-nassir-navab/nassir-navab/"> Nassir Navab</a><br>
		  <i>arXiv:2003.05848</i>.<br>
		  [<a href="https://arxiv.org/abs/2003.05848">Paper</a>]
		  [<a href="./paper/cpspp_arxiv_2020.html">bibtex</a>]
	</td></tr>

  	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/pdf/2004.06468.pdf"><img class="teaser_img" src='images/paper/2020eccv_self6d.png'/></a></div></td>
		<td class="pub_td2"><b>Self6D: Self-Supervised Monocular 6D Object
			Pose Estimation</b><br>
			<b>Gu Wang</b>*,
			<a href="https://campar.in.tum.de/Main/FabianManhardt"> Fabian Manhardt*</a>,
			<a href="https://www.aminer.cn/profile/jianzhun-shao/61775b3860a9653f448f03e7"> Jianzhun Shao</a>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/cv-nassir-navab/nassir-navab/"> Nassir Navab</a>,
			<a href="https://www.cs.cit.tum.de/camp/members/senior-research-scientists/federico-tombari/"> Federico Tombari</a><br>
			<i>In European Conference on Computer Vision (ECCV), 2020 (<b>oral</b>)</i>.<br>
			[<a href="https://arxiv.org/pdf/2004.06468.pdf">Paper</a>]
			[<a href="https://github.com/THU-DA-6D-Pose-Group/Self6D-Diff-Renderer">Diff-Renderer Code</a>]
			[<a href="https://www.bilibili.com/video/BV1iV411U77h/">Video</a>]
			[<a href="./paper/self6d_eccv2020.html">bibtex</a>]
	</td></tr>

    <tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_PFRL_Pose-Free_Reinforcement_Learning_for_6D_Pose_Estimation_CVPR_2020_paper.pdf"><img class="teaser_img" src='images/paper/2020cvpr_pfrl.png'/></a></div></td>
		<td class="pub_td2"><b>PFRL: Pose-Free Reinforcement Learning for 6D Pose Estimation</b><br>
			<a href="https://www.aminer.cn/profile/jianzhun-shao/61775b3860a9653f448f03e7">Jianzhun Shao</a>,
			<a href="https://scholar.google.com/citations?user=hBuJU48AAAAJ&hl=en"> Yuhang Jiang</a>,
			<b> Gu Wang</b>,
			<a href="https://scholar.google.com/citations?hl=en&user=CVVh3DIAAAAJ"> Zhigang Li</a>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a><br>
			<i>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i>.<br>
			[<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_PFRL_Pose-Free_Reinforcement_Learning_for_6D_Pose_Estimation_CVPR_2020_paper.pdf">Paper</a>]
			[<a href="https://www.youtube.com/watch?v=URq86Jz5nVU">Video</a>]
			[<a href="./paper/pfrl_cvpr2020.html">bibtex</a>]
	</td></tr>


 	<tr>
		<td class="pub_td1"><div class="teaser_img_div"><a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html"><img class="teaser_img" src='images/paper/2019iccv_cdpn.png'/></a></div></td>
		<td class="pub_td2"><b>CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation</b><br>
			<a href="https://scholar.google.com/citations?hl=en&user=CVVh3DIAAAAJ">Zhigang Li</a>,
			<b> Gu Wang</b>,
			<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a><br>
			<i>In IEEE International Conference on Computer Vision (ICCV), 2019 (<b>oral</b>)</i>.<br>
			[<a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html">Paper</a>]
			[<a href="https://github.com/LZGMatrix/BOP19_CDPN_2019ICCV">Code for BOP19/20</a>]
			[<a href="https://www.youtube.com/watch?v=zem03fZWLrQ">Oral Video</a>]
			[<a href="./paper/cdpn_iccv2019.html">bibtex</a>]
	</td></tr>

	<tr>
 	 <td class="pub_td1"><div class="teaser_img_div"><a href="./paper/Li2019_IJCV_DeepIM.pdf"><img class="teaser_img" src='images/paper/2018eccv_deepim_logo.png'/></a></div></td>
	  <td class="pub_td2"><b>DeepIM: Deep Iterative Matching for 6D Pose Estimation</b><br>
		<a href="http://yili.vision/">Yi Li</a>,
		<b> Gu Wang</b>,
		<a href="https://www.au.tsinghua.edu.cn/info/1166/2066.htm"> Xiangyang Ji</a>,
		<a href="https://yuxng.github.io/"> Yu Xiang</a>,
		<a href="http://homes.cs.washington.edu/~fox/"> Dieter Fox</a><br>
		<i>In European Conference on Computer Vision (ECCV), 2018 (<b>oral</b>)</i>.<br>
		<i>In International Journal of Computer Vision (IJCV), 2020</i>.<br>
		[<a href="./paper/Li2019_IJCV_DeepIM.pdf">Paper</a>]
		[<a href="https://github.com/liyi14/mx-DeepIM">Code</a>]
		[<a href="https://www.youtube.com/watch?v=zKe3D8w0NJE&feature=youtu.be">Slides</a>]
		[<a href="https://rse-lab.cs.washington.edu/projects/deepim/">Project Page</a>]
		[<a href="./paper/deepim_ijcv2019.html">bibtex</a>]

	</table>


	</div>


	<div class='section_div'>
		<h2>Experience</h2>
		<ul>
		<li> Broadband Network and Digital Media Lab, Tsinghua University, Beijing. (Aug. 2016 - Jan. 2022)<br/>
			<i>Ph.D. student, with advisor Prof. Xiangyang Ji</i></li><br/>
		<li> Computer Vision Group, CAMP Chair, Technical University of Munich. (Oct. 2019 - Sept. 2020)<br/>
			<i>Visiting Ph.D. student, with co-advisor Dr. Federico Tombari</i></li><br/>
		<li> Hangzhou Zhibei Tech. (Jul. 2018 - Aug. 2018)<br/>
			<i>Research intern, with Dr. Yaohe Cao</i></li><br/>
		<li> Department of Image Search, Baidu, Beijing. (Jul. 2015 - Oct. 2015)<br/>
			<i>Research intern, with Dr. Gaolin Fang</i></li><br/>
		</ul>
	</div>

	<div class='section_div'>
		<h2>Service</h2>
		<ul>
		<li> Co-organizer of the <a href="http://cmp.felk.cvut.cz/sixd/workshop_2022/">7th International Workshop on Recovering 6D Object Pose</a> and <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2022/">BOP Challenge 2022</a> @ ECCV 22 </i></li><br/>
		<li> Reviewer (Conference): CVPR 21/22, ICCV 19/21, ECCV 20/22, ICRA 22, ACM MM 22, IJCAI 22, IROS 20</i></li><br/>
		<li> Reviewer (Journal): TIP22, RAL 21/22, RAS 21, TIM 21, PR 22, Neurocomputing 22, JAS 21, ESWA 21/22, JVCIR 20/21/22</i></li><br/>
		</ul>
	</div>

		<hr>

</body>

</html>
